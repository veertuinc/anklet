#!/usr/bin/env bash
set -exo pipefail
YAML_CONFIG_FILE="${1}"
[ -z "${YAML_CONFIG_FILE}" ] && echo "ARG 1 (Yaml Config File; org-receiver-config.yml) is not set" && exit 1

SCRIPT_DIR=$(cd $(dirname "${BASH_SOURCE[0]}") && pwd)
cd $SCRIPT_DIR

# Function to check for duplicate JSON keys in the log file
check_duplicate_keys() {
    local log_file="/tmp/$(basename ${YAML_CONFIG_FILE}).log"

    if [ ! -f "$log_file" ]; then
        echo "Log file not found: $log_file"
        return 1
    fi

    echo "Checking for duplicate JSON keys in log file..."

    # Use Python to parse the log file and detect duplicate keys
    LOG_FILE="$log_file" python3 << 'EOF'
import sys
import re
import os

def detect_duplicate_keys_in_log(log_file):
    """Check for duplicate keys within each JSON object in log file"""
    lines_with_duplicates = []

    try:
        with open(log_file, 'r') as f:
            for line_num, line in enumerate(f, 1):
                original_line = line.strip()
                if not original_line:
                    continue

                # Try to detect duplicate keys within this single JSON object
                duplicates = detect_duplicates_in_line(original_line)
                if duplicates:
                    lines_with_duplicates.append((line_num, original_line, duplicates))

    except Exception as e:
        print('Error reading log file: {}'.format(e), file=sys.stderr)
        return False

    if lines_with_duplicates:
        print('\nâŒ Duplicate JSON keys found within individual objects:', file=sys.stderr)
        print('=' * 80, file=sys.stderr)

        for line_num, original_line, dups in lines_with_duplicates:
            print('ðŸ“ Line {} - Duplicate keys: {}'.format(line_num, ', '.join(sorted(dups))), file=sys.stderr)
            print('   Object: {}'.format(original_line), file=sys.stderr)
            print('', file=sys.stderr)

        print('ðŸ“Š Summary: {} lines contain duplicate keys'.format(len(lines_with_duplicates)), file=sys.stderr)
        return False
    else:
        print('âœ… No duplicate JSON keys found within individual objects.')
        return True

def detect_duplicates_in_line(json_str):
    """Detect duplicate keys by parsing JSON structure and checking each object individually"""
    try:
        import json

        def check_object_for_duplicates(obj, path="root"):
            """Check a single object for duplicate keys"""
            if not isinstance(obj, dict):
                return []

            duplicates = []
            seen_keys = set()

            for key in obj.keys():
                if key in seen_keys:
                    duplicates.append(key)
                else:
                    seen_keys.add(key)

            return duplicates

        # Parse the JSON and check each object individually
        parsed = json.loads(json_str)
        all_duplicates = []

        # Check root level
        root_dups = check_object_for_duplicates(parsed, "root")
        all_duplicates.extend(root_dups)

        # Recursively check nested objects
        def check_nested(obj, path="root"):
            dups = []
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = "{}.{}".format(path, key)
                    # Check if this value is an object
                    if isinstance(value, dict):
                        nested_dups = check_object_for_duplicates(value, current_path)
                        dups.extend(nested_dups)
                        # Recursively check deeper
                        deeper_dups = check_nested(value, current_path)
                        dups.extend(deeper_dups)
                    elif isinstance(value, list):
                        # Check objects within arrays
                        for i, item in enumerate(value):
                            if isinstance(item, dict):
                                array_path = "{}[{}]".format(current_path, i)
                                array_dups = check_object_for_duplicates(item, array_path)
                                dups.extend(array_dups)
                                # Recursively check array items
                                array_nested_dups = check_nested(item, array_path)
                                dups.extend(array_nested_dups)
            return dups

        nested_duplicates = check_nested(parsed)
        all_duplicates.extend(nested_duplicates)

        return list(set(all_duplicates))  # Remove duplicates in our list

    except (json.JSONDecodeError, ValueError):
        # If JSON parsing fails, fall back to simple duplicate detection
        return []

def find_duplicates_in_object(obj, duplicates, path=""):
    """Recursively find duplicate keys within objects"""
    if isinstance(obj, dict):
        # Check for duplicates in current object
        seen_keys = set()
        for key in obj.keys():
            if key in seen_keys:
                if key not in duplicates:
                    duplicates.append(key)
            else:
                seen_keys.add(key)

        # Recursively check nested objects
        for key, value in obj.items():
            new_path = "{}.{}".format(path, key) if path else key
            find_duplicates_in_object(value, duplicates, new_path)

    elif isinstance(obj, list):
        # Check each item in the array
        for i, item in enumerate(obj):
            new_path = "{}[{}]".format(path, i) if path else "[{}]".format(i)
            find_duplicates_in_object(item, duplicates, new_path)

def detect_duplicates_regex_fallback(json_str):
    """Fallback method using regex with better object boundary detection"""
    duplicates = []

    # Split JSON into top-level objects by tracking brace levels
    objects = split_json_objects(json_str)

    for obj_str in objects:
        obj_duplicates = find_duplicates_in_json_string(obj_str)
        duplicates.extend(obj_duplicates)

    return list(set(duplicates))

def split_json_objects(json_str):
    """Split JSON string into individual object strings at the top level"""
    objects = []
    brace_level = 0
    current_obj = ""
    in_string = False
    escape_next = False

    i = 0
    while i < len(json_str):
        char = json_str[i]

        # Handle string literals
        if char == '"' and not escape_next:
            in_string = not in_string
        elif char == '\\' and in_string:
            escape_next = True
            i += 1
            continue
        elif escape_next:
            escape_next = False

        # Only process structure when not in a string
        if not in_string:
            if char == '{':
                if brace_level == 0:
                    current_obj = ""
                brace_level += 1
            elif char == '}':
                brace_level -= 1
                if brace_level == 0 and current_obj:
                    current_obj += char
                    objects.append(current_obj.strip())
                    current_obj = ""

        if brace_level > 0:
            current_obj += char

        i += 1

    return objects

def find_duplicates_in_json_string(json_str):
    """Find duplicates in a single JSON object string using regex with context"""
    # This is a simplified approach - for complex nested structures,
    # the JSON parsing approach above is preferred
    key_pattern = re.compile(r'"([^"]+)"\s*:')
    keys = key_pattern.findall(json_str)

    # For now, just return empty list to avoid false positives
    # The JSON parsing approach above handles this correctly
    return []

if __name__ == '__main__':
    log_file = os.environ.get('LOG_FILE')
    if not log_file:
        print("ERROR: LOG_FILE environment variable not set", file=sys.stderr)
        sys.exit(1)

    success = detect_duplicate_keys_in_log(log_file)
    sys.exit(0 if success else 1)
EOF

    return $?
}

# Cleanup function to check logs and clean up
cleanup() {
    echo ""
    echo "Performing cleanup and validation..."

    # Kill the Go process if it's still running
    if kill -0 $go_pid 2>/dev/null; then
        echo "Stopping Go process (PID: $go_pid)..."
        kill -SIGINT $go_pid 2>/dev/null || true
        wait $go_pid 2>/dev/null || true
    fi

    # Check for duplicate keys in the log file
    check_duplicate_keys

    # Clean up the log file
    log_file="/tmp/$(basename ${YAML_CONFIG_FILE}).log"
    if [ -f "$log_file" ]; then
        echo "Log file saved at: $log_file"
        echo "You can review it or remove it manually."
    fi
}

# we use JQ here instead of pretty print in the logging.go so that we can ensure valid JSON is output from go
# Feed stdout through jq so we still validate JSON, but only surface time/level/msg/attributes for readability (errors red, warnings yellow, debug grey).
LOG_LEVEL=${LOG_LEVEL:-dev} go run main.go -c ${YAML_CONFIG_FILE} 2>&1 \
    | tee /tmp/$(basename ${YAML_CONFIG_FILE}).log \
    | jq -r '
        if type == "object" then
            (.level // .severity // "") as $level |
            (.time // .ts // .timestamp // "") as $time |
            (.msg // "") as $msg |
            (.attributes // {}) as $attrs |
            ($level | tostring) as $levelStr |
            ($time | tostring) as $timeStr |
            ($msg | tostring) as $msgStr |
            ($attrs | tojson) as $attrsJson |
            ("time=" + $timeStr + " level=" + $levelStr + " msg=" + $msgStr + " attributes=" + $attrsJson) as $line |
            ($levelStr | ascii_upcase) as $levelUpper |
            (if $levelUpper == "ERROR" then
                "\u001b[31m" + $line + "\u001b[0m"
            elif ($levelUpper == "WARN" or $levelUpper == "WARNING") then
                "\u001b[33m" + $line + "\u001b[0m"
            elif $levelUpper == "DEBUG" then
                "\u001b[90m" + $line + "\u001b[0m"
            else
                "\u001b[33mtime\u001b[0m=\u001b[37m" + $timeStr + "\u001b[0m level=" + $levelStr + " msg=" + $msgStr + " attributes=" + $attrsJson
            end)
        else
            .
        end' &
go_pid=$!

# Set up trap to run cleanup on exit
trap cleanup EXIT

echo "Anklet is running... Press Ctrl+C to stop and check for duplicate keys."
wait $go_pid
